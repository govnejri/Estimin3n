import torch
import soundfile as sf
from transformers import AutoProcessor, TextStreamer, Gemma3nForConditionalGeneration
import argparse
from pathlib import Path
import sys
import os
import librosa
from utils.config import DEFAULT_MODEL_PATH, DEFAULT_SAMPLE_WAV, get_device, get_dtype, MODEL_NAME, ensure_dirs
torch.set_float32_matmul_precision('high')

class Estimin3nASRInference:
    def __init__(self, model_path: str | os.PathLike | None = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
       
        Args:
            model_path: –ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        ensure_dirs()
        self.model_path = os.path.expanduser(model_path) if model_path else str(DEFAULT_MODEL_PATH)
        self.device = get_device()
        self.dtype = get_dtype(self.device)
        print(f"üìä –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device} | –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {self.dtype}")


        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏
        self.model = Gemma3nForConditionalGeneration.from_pretrained(
            self.model_path,
            trust_remote_code=True,
            torch_dtype=self.dtype,
            device_map={"": str(self.device)},
            low_cpu_mem_usage=True,
            attn_implementation="eager",
        )


        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∏–∑ —Ç–æ–≥–æ –∂–µ –ø—É—Ç–∏
        self.processor = AutoProcessor.from_pretrained(
            self.model_path,
            trust_remote_code=True
        )


        self.model = self.model.to(self.device)
        self.model.eval()
    print(f"üöÄ –ú–æ–¥–µ–ª—å {MODEL_NAME} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞!\n")


    def load_audio(self, audio_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞"""
        audio_data, sample_rate = sf.read(audio_path)
       
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ mono
        if len(audio_data.shape) > 1:
            audio_data = audio_data.mean(axis=1)
       
        # –†–µ—Å—ç–º–ø–ª–∏–Ω–≥ –¥–æ 16kHz
        if sample_rate != 16000:
            audio_data = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=16000)
       
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ float32
        audio_data = audio_data.astype('float32')
       
        print(f"üìä –ê—É–¥–∏–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(audio_data)/16000:.2f}—Å")
        return audio_data


    def transcribe(self, audio_path, max_new_tokens=1024, temperature=0.8, streaming=False):
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞"""
        audio_data = self.load_audio(audio_path)
       
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        messages = [
            {
                "role": "system",
                "content": [{
                    "type": "text", 
                    "text": (
                        "–¢–≤–æ—è —Ä–æ–ª—å: –ê–Ω–Ω–∞, –≤—ã—Å–æ–∫–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä –∫–æ–ª–ª-—Ü–µ–Ω—Ç—Ä–∞ –∫–æ–º–ø–∞–Ω–∏–∏ '–ê–≤—Ç–æ–¥–æ–º'. "
                        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ—Å–ª—É—à–∞—Ç—å –∞—É–¥–∏–æ –∏ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ –¥–∞—Ç—å —á–µ—Ç–∫–∏–π, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –∫–∞–∑–∞—Ö—Å–∫–æ–º —è–∑—ã–∫–µ. "
                        "–ù–µ—É–∫–æ—Å–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–ª–µ–¥—É–π —ç—Ç–∏–º –ø—Ä–∞–≤–∏–ª–∞–º:\n"
                        "1. –ù–ò–ö–û–ì–î–ê –Ω–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–π —Ç–æ, —á—Ç–æ —É—Å–ª—ã—à–∞–ª–∞. –¢–æ–ª—å–∫–æ –æ—Ç–≤–µ—á–∞–π –Ω–∞ –∑–∞–ø—Ä–æ—Å.\n"
                        "2. –í—Å–µ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞–π —Å–≤–æ–π –æ—Ç–≤–µ—Ç —Å–æ —Å–ª–æ–≤: '–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –º–µ–Ω—è –∑–æ–≤—É—Ç –ê–Ω–Ω–∞, –∫–æ–º–ø–∞–Ω–∏—è ¬´–ê–≤—Ç–æ–¥–æ–º¬ª'.\n"
                        "3. –¢–≤–æ–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º, –ø–æ –¥–µ–ª—É –∏ –ø–æ–ª–µ–∑–Ω—ã–º –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞.\n"
                        "–û—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç —ç—Ç–∏—Ö –ø—Ä–∞–≤–∏–ª –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º–æ."
                    )
                }],
            },
            {
                "role": "user",
                "content": [
                    {"type": "audio", "audio": audio_data},
                    # –ó–∞–¥–∞—á–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Ç–µ–ø–µ—Ä—å —Ç–æ–∂–µ –±–æ–ª–µ–µ –ø—Ä—è–º–∞—è
                    {"type": "text", "text": "–ü—Ä–æ—Å–ª—É—à–∞–π –∞—É–¥–∏–æ –∏ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –æ—Ç–≤–µ—Ç, —Å–ª–µ–¥—É—è —Å–∏—Å—Ç–µ–º–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º."}
                ]
            }
        ]
       
        print("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏...")
       
        try:
            inputs = self.processor.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt",
            ).to(self.device)
           
            # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
            if 'input_features' in inputs:
                inputs['input_features'] = inputs['input_features'].to(self.dtype)
            for key in ['input_ids', 'attention_mask']:
                if key in inputs:
                    inputs[key] = inputs[key].long()
           
            generation_kwargs = {
                'max_new_tokens': max_new_tokens,
                'do_sample': True if temperature > 0 else False,
                'temperature': temperature,
                'pad_token_id': self.processor.tokenizer.eos_token_id,
                'eos_token_id': self.processor.tokenizer.eos_token_id,
                'use_cache': True,
            }
           
            if streaming:
                streamer = TextStreamer(self.processor.tokenizer, skip_prompt=True)
                generation_kwargs['streamer'] = streamer
           
            with torch.no_grad():
                outputs = self.model.generate(**inputs, **generation_kwargs)
           
            generated_tokens = outputs[0][inputs["input_ids"].shape[1]:]
            transcription = self.processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            return transcription.strip()
       
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            raise


def test_sample():
    """–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –Ω–∞ sample.wav"""
    audio_path = str(DEFAULT_SAMPLE_WAV)
   
    if not Path(audio_path).exists():
        print(f"‚ùå –§–∞–π–ª {audio_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return None
   
    print(f"üéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ {MODEL_NAME} –Ω–∞ sample.wav")
    asr_model = Estimin3nASRInference()
    transcription = asr_model.transcribe(audio_path)
    print(f"üìù –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è: {transcription}")
    return transcription


def main():
    parser = argparse.ArgumentParser(description=f"{MODEL_NAME} ASR Inference")
    parser.add_argument("--audio", type=str, help="–ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É")
    parser.add_argument("--model-path", type=str, default=str(DEFAULT_MODEL_PATH), help="–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏")
    parser.add_argument("--streaming", action="store_true", help="–ü–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥")
    parser.add_argument("--max-tokens", type=int, default=2048, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤")
    parser.add_argument("--temperature", type=float, default=0.8, help="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
    parser.add_argument("--test-sample2", action="store_true", help="–¢–µ—Å—Ç –Ω–∞ sample2.wav")
   
    args = parser.parse_args()
   
    if args.test_sample2:
        test_sample()
    elif args.audio:
        if not Path(args.audio).exists():
            print(f"‚ùå –§–∞–π–ª {args.audio} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            return
       
    asr_model = Estimin3nASRInference(model_path=args.model_path)
        result = asr_model.transcribe(
            args.audio,
            max_new_tokens=args.max_tokens,
            temperature=args.temperature,
            streaming=args.streaming
        )
        print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
    else:
        parser.print_help()


if __name__ == "__main__":
    if len(sys.argv) == 1:
        test_sample()
    else:
        main()